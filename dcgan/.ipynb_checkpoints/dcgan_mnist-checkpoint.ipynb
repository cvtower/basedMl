{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度卷积GAN之MNIST数据生成\n",
    "\n",
    "该代码将在之前GAN生成MNIST数据的基础上进行改进，对generator改为transposed convolution结构，对discriminator采用卷积结构，提高模型生成效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow Version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 10\n",
    "img = mnist.train.images[i]\n",
    "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "print(\"Label: {}\".format(mnist.train.labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "- inputs\n",
    "- generator\n",
    "- discriminator\n",
    "- loss\n",
    "- optimizer\n",
    "- train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(noise_dim, image_height, image_width, image_depth):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    :param noise_dim: 噪声图片的size\n",
    "    :param image_height: 真实图像的height\n",
    "    :param image_width: 真实图像的width\n",
    "    :param image_depth: 真实图像的depth\n",
    "    \"\"\" \n",
    "    inputs_real = tf.placeholder(tf.float32, [None, image_height, image_width, image_depth], name='inputs_real')\n",
    "    inputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name='inputs_noise')\n",
    "    \n",
    "    return inputs_real, inputs_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generator(noise_img, output_dim, is_train=True, alpha=0.01):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    :param noise_img: 噪声信号，tensor类型\n",
    "    :param output_dim: 生成图片的depth\n",
    "    :param is_train: 是否为训练状态，该参数主要用于作为batch_normalization方法中的参数使用\n",
    "    :param alpha: Leaky ReLU系数\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=(not is_train)):\n",
    "        # 100 x 1 to 4 x 4 x 512\n",
    "        # 全连接层\n",
    "        layer1 = tf.layers.dense(noise_img, 4*4*512)\n",
    "        layer1 = tf.reshape(layer1, [-1, 4, 4, 512])\n",
    "        # batch normalization\n",
    "        layer1 = tf.layers.batch_normalization(layer1, training=is_train)\n",
    "        # Leaky ReLU\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1)\n",
    "        # dropout\n",
    "        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n",
    "        \n",
    "        # 4 x 4 x 512 to 7 x 7 x 256\n",
    "        layer2 = tf.layers.conv2d_transpose(layer1, 256, 4, strides=1, padding='valid')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=is_train)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2)\n",
    "        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n",
    "        \n",
    "        # 7 x 7 256 to 14 x 14 x 128\n",
    "        layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=is_train)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3)\n",
    "        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n",
    "        \n",
    "        # 14 x 14 x 128 to 28 x 28 x 1\n",
    "        logits = tf.layers.conv2d_transpose(layer3, output_dim, 3, strides=2, padding='same')\n",
    "        # MNIST原始数据集的像素范围在0-1，这里的生成图片范围为(-1,1)\n",
    "        # 因此在训练时，记住要把MNIST像素范围进行resize\n",
    "        outputs = tf.tanh(logits)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_discriminator(inputs_img, reuse=False, alpha=0.01):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    @param inputs_img: 输入图片，tensor类型\n",
    "    @param alpha: Leaky ReLU系数\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # 28 x 28 x 1 to 14 x 14 x 128\n",
    "        # 第一层不加入BN\n",
    "        layer1 = tf.layers.conv2d(inputs_img, 128, 3, strides=2, padding='same')\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1)\n",
    "        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n",
    "        \n",
    "        # 14 x 14 x 128 to 7 x 7 x 256\n",
    "        layer2 = tf.layers.conv2d(layer1, 256, 3, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2)\n",
    "        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n",
    "        \n",
    "        # 7 x 7 x 256 to 4 x 4 x 512\n",
    "        layer3 = tf.layers.conv2d(layer2, 512, 3, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3)\n",
    "        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n",
    "        \n",
    "        # 4 x 4 x 512 to 4*4*512 x 1\n",
    "        flatten = tf.reshape(layer3, (-1, 4*4*512))\n",
    "        logits = tf.layers.dense(flatten, 1)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        \n",
    "        return logits, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(inputs_real, inputs_noise, image_depth, smooth=0.1):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    @param inputs_real: 输入图片，tensor类型\n",
    "    @param inputs_noise: 噪声图片，tensor类型\n",
    "    @param image_depth: 图片的depth（或者叫channel）\n",
    "    @param smooth: label smoothing的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    g_outputs = get_generator(inputs_noise, image_depth, is_train=True)\n",
    "    d_logits_real, d_outputs_real = get_discriminator(inputs_real)\n",
    "    d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, reuse=True)\n",
    "    \n",
    "    # 计算Loss\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                                    labels=tf.ones_like(d_outputs_fake)*(1-smooth)))\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                                         labels=tf.ones_like(d_outputs_real)*(1-smooth)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                         labels=tf.zeros_like(d_outputs_fake)))\n",
    "    d_loss = tf.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(g_loss, d_loss, beta1=0.4, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    @param g_loss: Generator的Loss\n",
    "    @param d_loss: Discriminator的Loss\n",
    "    @learning_rate: 学习率\n",
    "    \"\"\"\n",
    "    \n",
    "    train_vars = tf.trainable_variables()\n",
    "    \n",
    "    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    "    \n",
    "    # Optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "        d_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "    \n",
    "    return g_opt, d_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数，用来在迭代中显示图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(samples):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=25, sharex=True, sharey=True, figsize=(50,2))\n",
    "    for img, ax in zip(samples, axes):\n",
    "        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout(pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generator_output(sess, n_images, inputs_noise, output_dim):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    @param sess: TensorFlow session\n",
    "    @param n_images: 展示图片的数量\n",
    "    @param inputs_noise: 噪声图片\n",
    "    @param output_dim: 图片的depth（或者叫channel）\n",
    "    @param image_mode: 图像模式：RGB或者灰度\n",
    "    \"\"\"\n",
    "    cmap = 'Greys_r'\n",
    "    noise_shape = inputs_noise.get_shape().as_list()[-1]\n",
    "    # 生成噪声图片\n",
    "    examples_noise = np.random.uniform(-1, 1, size=[n_images, noise_shape])\n",
    "\n",
    "    samples = sess.run(get_generator(inputs_noise, output_dim, False),\n",
    "                       feed_dict={inputs_noise: examples_noise})\n",
    "\n",
    "    \n",
    "    result = np.squeeze(samples, -1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "batch_size = 64\n",
    "noise_size = 100\n",
    "epochs = 100\n",
    "n_samples = 25\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(noise_size, data_shape, batch_size, n_samples):\n",
    "    \"\"\"\n",
    "    @Author: Nelson Zhao\n",
    "    --------------------\n",
    "    @param noise_size: 噪声size\n",
    "    @param data_shape: 真实图像shape\n",
    "    @batch_size:\n",
    "    @n_samples: 显示示例图片数量\n",
    "    \"\"\"\n",
    "    \n",
    "    # 存储loss\n",
    "    losses = []\n",
    "    steps = 0\n",
    "    \n",
    "    inputs_real, inputs_noise = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3])\n",
    "    g_loss, d_loss = get_loss(inputs_real, inputs_noise, data_shape[-1])\n",
    "    g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # 迭代epoch\n",
    "        for e in range(epochs):\n",
    "            for batch_i in range(mnist.train.num_examples//batch_size):\n",
    "                steps += 1\n",
    "                batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "                batch_images = batch[0].reshape((batch_size, data_shape[1], data_shape[2], data_shape[3]))\n",
    "                # scale to -1, 1\n",
    "                batch_images = batch_images * 2 - 1\n",
    "\n",
    "                # noise\n",
    "                batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))\n",
    "\n",
    "                # run optimizer\n",
    "                _ = sess.run(g_train_opt, feed_dict={inputs_real: batch_images,\n",
    "                                                     inputs_noise: batch_noise})\n",
    "                _ = sess.run(d_train_opt, feed_dict={inputs_real: batch_images,\n",
    "                                                     inputs_noise: batch_noise})\n",
    "                \n",
    "                if steps % 101 == 0:\n",
    "                    train_loss_d = d_loss.eval({inputs_real: batch_images,\n",
    "                                                inputs_noise: batch_noise})\n",
    "                    train_loss_g = g_loss.eval({inputs_real: batch_images,\n",
    "                                                inputs_noise: batch_noise})\n",
    "                    losses.append((train_loss_d, train_loss_g))\n",
    "                    # 显示图片\n",
    "                    samples = show_generator_output(sess, n_samples, inputs_noise, data_shape[-1])\n",
    "                    plot_images(samples)\n",
    "                    print(\"Epoch {}/{}....\".format(e+1, epochs), \n",
    "                          \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}....\". format(train_loss_g))\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(noise_size, [-1, 28, 28, 1], batch_size, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
