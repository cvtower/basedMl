{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"./img/oscon.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we show how to take [GloVe](http://nlp.stanford.edu/projects/glove/) word vectors trained based on Common Crawl web data, and incorporate them into a our text classification.\n",
    "\n",
    "Start by setting the path to which you downloaded the pre-trained vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "DATA_DIR = os.path.join(os.environ[\"HOME\"], \"data\") if not 'win32' in sys.platform else \"c:\\\\tmp\\\\data\"\n",
    "path_to_glove = os.path.join(DATA_DIR,\"glove.840B.300d.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "digit_to_word_map[0]=\"PAD_TOKEN\"\n",
    "times_steps = 6\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,times_steps+1))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "                                     rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "                                      rand_seq_len)\n",
    "\n",
    "    #Padding\n",
    "    if rand_seq_len<times_steps:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(times_steps-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                   [0]*(times_steps-rand_seq_len))\n",
    "\n",
    "    even_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                               r in rand_even_ints])]\n",
    "    odd_sentences+=[\" \".join([digit_to_word_map[r] for\n",
    "                              r in rand_odd_ints])] \n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "seqlens = seqlens + seqlens\n",
    "#Map from words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "#Inverse map    \n",
    "index2word_map = dict([(index,word) for word,index in\n",
    "                       word2index_map.items()])            \n",
    "vocabulary_size = len(index2word_map)\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding\n",
    "\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "def get_sentence_batch(batch_size,data_x,\n",
    "                       data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()]\n",
    "         for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are 2.2 million words in the vocabulary of the pre-trained GloVe embeddings we downloaded â€” we only take the GloVe vectors for words that appear in our own, tiny vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def get_glove(path_to_glove,word2index_map):\n",
    "        \n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0 \n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open(\"glove.840B.300d.txt\") as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\")) \n",
    "                if word in word2index_map:\n",
    "                    print(word)\n",
    "                    count_all_words+=1\n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs/=np.linalg.norm(coefs) \n",
    "                    embedding_weights[word] = coefs\n",
    "                if count_all_words==len(word2index_map)-1:\n",
    "                    break\n",
    "    return embedding_weights \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output of our function is a dictionary, mapping from each word to its vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n",
      "three\n",
      "four\n",
      "five\n",
      "six\n",
      "seven\n",
      "eight\n",
      "nine\n",
      "[  1.17593138e-02   2.41619311e-02  -3.16584334e-02  -5.28343767e-03\n",
      "   8.28812942e-02  -3.12930681e-02  -3.91418897e-02  -6.75903335e-02\n",
      "  -1.73309119e-03   5.77146113e-01  -2.77764332e-02   5.19448891e-03\n",
      "   1.55027825e-02  -3.10212206e-02   2.37900410e-02  -2.31984966e-02\n",
      "  -8.30291770e-03   2.57777989e-01   1.14304852e-02  -3.73650864e-02\n",
      "  -4.05141823e-02  -3.59558244e-03  -1.82699859e-02   3.16258147e-02\n",
      "   3.49249691e-02  -1.50415087e-02  -1.67050064e-02  -4.49268371e-02\n",
      "   3.48510258e-02   4.65514027e-02   1.28767211e-02   1.04152925e-01\n",
      "   1.81321036e-02   4.34827730e-02   2.09269281e-02  -2.18196809e-02\n",
      "   9.25960764e-03  -2.03532185e-02  -2.91182790e-02  -7.11243674e-02\n",
      "   5.14338119e-03   7.40820915e-02  -1.65231936e-02  -2.20741294e-02\n",
      "   2.70348303e-02  -1.21688265e-02  -5.49874231e-02  -4.02771309e-03\n",
      "   1.02023799e-02  -1.84365753e-02  -3.45443822e-02   5.16534634e-02\n",
      "   1.65521186e-02   1.56047801e-02   6.82993140e-03   7.10199773e-03\n",
      "   2.45120712e-02   6.05440103e-02   7.66244158e-02  -2.35834345e-02\n",
      "   1.52633376e-02  -1.01978136e-02  -4.09404449e-02   8.81181881e-02\n",
      "   4.60620746e-02  -3.12647969e-02  -1.04553089e-03   3.01795769e-02\n",
      "  -1.99039076e-02   5.47916926e-02   3.91418897e-02  -3.45791802e-02\n",
      "   3.20846960e-02   2.58060731e-02  -3.35939974e-02   2.38552857e-02\n",
      "   1.26063945e-02  -5.62444478e-02  -5.16817346e-03   4.14602198e-02\n",
      "  -7.47366948e-03   1.97645016e-02  -6.31037429e-02  -2.82788109e-02\n",
      "   6.23425655e-03  -4.48507182e-02  -6.21685795e-02  -4.98766601e-02\n",
      "   3.34526375e-02  -4.43070196e-02  -5.51309586e-02   4.50247005e-02\n",
      "  -1.87032055e-02  -1.83571950e-02  -2.80635059e-02   1.29263066e-02\n",
      "   1.85237844e-02  -9.08497255e-03   1.87184270e-02   1.70196984e-02\n",
      "  -1.45580526e-02  -4.81237797e-03  -2.11398397e-03  -5.35020372e-03\n",
      "   1.68839912e-03  -2.08714709e-01   9.66694672e-03   1.32679660e-02\n",
      "  -2.69761104e-02   1.64227188e-02   8.04912001e-02  -3.89352851e-02\n",
      "  -1.59866717e-02   3.88287231e-02  -2.33529080e-02   1.21266358e-02\n",
      "  -7.75313079e-02   8.92490819e-02  -3.85590494e-02   2.26461012e-02\n",
      "   1.53314080e-02  -8.84857327e-02   5.66924550e-02  -5.09031629e-03\n",
      "  -9.03343037e-03   1.96705516e-02  -7.38711357e-02  -8.03628862e-02\n",
      "  -4.20930842e-03   2.57386528e-02  -3.47227156e-02   2.43185158e-03\n",
      "   5.07096061e-03   5.20405779e-03   1.62365548e-02  -1.44871545e-03\n",
      "   2.98185609e-02   6.75881607e-03   4.01205458e-02  -1.82404090e-02\n",
      "  -3.65734637e-01   4.89328019e-02   3.81414890e-02   6.91235662e-02\n",
      "  -1.14383148e-02  -1.83360986e-02  -2.96402294e-02  -1.62724406e-02\n",
      "   8.62109009e-03  -2.96750236e-02  -3.04644760e-02   2.08223201e-02\n",
      "   1.01066893e-03   1.88391283e-02   1.13715483e-02  -1.25302770e-03\n",
      "  -4.55488265e-02   1.62944049e-02  -4.02205884e-02  -2.71087717e-02\n",
      "  -1.31063797e-02   6.83515100e-03   1.26607651e-02  -2.29483973e-02\n",
      "   3.86134163e-03  -5.37782349e-02  -8.21309909e-03  -3.53316573e-05\n",
      "   2.54950766e-02   2.15421766e-02  -1.36046240e-03  -6.20011203e-02\n",
      "   3.90505530e-02  -2.98990309e-02  -9.34638306e-02   9.76372510e-03\n",
      "  -3.89657319e-02  -1.80888269e-02  -1.56889446e-02  -2.55820677e-02\n",
      "   3.74346823e-02   5.81256440e-03  -2.95575876e-02  -4.37459238e-02\n",
      "  -5.08487895e-02   4.93438356e-03  -1.82930380e-02  -4.58924435e-02\n",
      "  -2.55864188e-02  -5.15599502e-03  -3.48423272e-02   1.33034149e-02\n",
      "  -1.49164582e-02   4.83695306e-02   6.94715306e-02  -1.46239484e-02\n",
      "  -5.32345399e-02  -1.10046612e-02  -1.50051890e-02  -2.07947008e-02\n",
      "  -3.86221148e-02  -3.52163911e-02  -4.16842215e-02  -1.14211328e-02\n",
      "   5.95849268e-02   2.73871468e-03  -3.19498568e-03  -3.32786515e-02\n",
      "   5.50135178e-03   5.24603128e-02  -3.32308076e-02   1.90942306e-02\n",
      "  -8.22049305e-02  -8.11175406e-02  -2.46621314e-02   2.55885925e-02\n",
      "  -3.63820828e-02   1.21981855e-02  -3.51402760e-02  -1.01042977e-02\n",
      "  -4.18342836e-02  -2.29701437e-02   2.91530751e-02   5.44567741e-02\n",
      "   3.56404781e-02  -6.48131296e-02   4.24410515e-02   6.77034259e-02\n",
      "  -5.66011108e-02  -6.06788462e-03  -3.24500613e-02  -6.91518362e-04\n",
      "   2.11198311e-02   5.98480776e-02  -6.38453439e-02  -6.30906969e-02\n",
      "   5.21036498e-02  -4.40025469e-03  -4.78649810e-02   7.69419372e-02\n",
      "  -3.00925854e-03   7.53456429e-02   3.44769657e-02   9.48361158e-02\n",
      "   5.13946675e-02  -7.28750825e-02   2.00824551e-02  -6.55547297e-03\n",
      "  -4.65731509e-02   2.27352660e-02   3.56665775e-02  -2.21589468e-02\n",
      "   5.36412233e-03   2.05574301e-03   1.11645076e-03   3.73520367e-02\n",
      "   4.94656265e-02   7.48715317e-03   1.36022316e-02   2.32702661e-02\n",
      "   6.09267727e-02  -2.16902792e-02  -6.96107149e-02  -2.06178892e-02\n",
      "  -2.14495305e-02   1.99373974e-03   6.13682605e-02   4.14188989e-02\n",
      "   8.62870142e-02  -2.16424335e-02   3.18541639e-02  -2.63149738e-02\n",
      "  -1.51126236e-02  -9.59757045e-02  -8.01258313e-04   5.84301129e-02\n",
      "  -2.40336172e-03   2.98838057e-02   7.40233660e-02   2.65759472e-02\n",
      "   5.51113859e-02   1.00190453e-02  -3.72672230e-02  -3.31503414e-02\n",
      "  -1.43131707e-03  -1.19735301e-02   4.90806848e-02  -3.34287137e-02\n",
      "   2.75828764e-02   2.46447343e-02  -4.88871336e-02  -7.47084292e-03\n",
      "  -1.79739967e-02  -2.93270573e-02  -2.47295499e-02  -1.57572310e-02\n",
      "  -1.58822816e-02  -5.89324906e-02   9.57625732e-03   8.19678826e-04]\n"
     ]
    }
   ],
   "source": [
    "word2embedding_dict = get_glove(path_to_glove,word2index_map)\n",
    "print(word2embedding_dict[\"one\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We place these vectors in a matrix, where each row index corresponds to the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOVE_SIZE = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2index_map),GLOVE_SIZE))\n",
    "\n",
    "for word,index in word2index_map.items():\n",
    "    if not word == \"pad_token\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that for the \"pad_token\" word, we set the corresponding vector to 0. The padded vectors are ignored by dynamic_rnn(), as we saw above.\n",
    "\n",
    "Now, back to TensorFlow. First, we create the exact same placeholders we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size=128;num_classes = 2\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we create an embedding_placeholder, to which we feed the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size,\n",
    "                                                    GLOVE_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Our embeddings are initialized with the content of embedding_placeholder, using the assign function to assign initial values to the embeddings variable.   We set trainable=True, to tell TensorFlow we want to update the values of the word vectors, by optimizing them for the task at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED = True\n",
    "\n",
    "if PRE_TRAINED:\n",
    "    with tf.device('/cpu:0'):\n",
    "            embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, GLOVE_SIZE]),\n",
    "                                     trainable=True)\n",
    "            embedding_init = embeddings.assign(embedding_placeholder)\n",
    "            embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "else:\n",
    "    with tf.device('/cpu:0'):\n",
    "            embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size,\n",
    "                                   embedding_dimension],\n",
    "                                  -1.0, 1.0))\n",
    "            embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Next, we repeat the exact same LSTM network structure we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 32\n",
    "with tf.variable_scope(\"lstm\"):\n",
    " \n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(hidden_layer_size)\n",
    "    _, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                        sequence_length = _seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "last_state = states[1]\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],mean=0,stddev=.01))\n",
    "b = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "\n",
    "#extract the final state and use in a linear layer\n",
    "final_pred = tf.matmul(last_state,W) + b\n",
    "#\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits  = final_pred,labels = _labels)                         \n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),\n",
    "                              tf.argmax(final_pred,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                   tf.float32)))*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Finally, we are ready to train. We initialize the embedding_placeholder by feeding it our embedding_matrix. Importantly, we do so after calling tf.global_variables_initializer() â€” doing so in the reverse order, would overrun the pre-trained vectors with a default initializer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 51.56250\n",
      "Accuracy at 100: 48.43750\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embedding_init, feed_dict=\n",
    "         {embedding_placeholder: embedding_matrix})\n",
    "for step in range(1000):\n",
    "    x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                       train_x,train_y,\n",
    "                                                       train_seqlens)\n",
    "    sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                   _seqlens:seqlen_batch})\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                           _labels:y_batch,\n",
    "                                           _seqlens:seqlen_batch})\n",
    "        print(\"Accuracy at %d: %.5f\" % (step, acc)) \n",
    "\n",
    "for test_batch in range(5):\n",
    "    x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                    test_x,test_y,\n",
    "                                                    test_seqlens)\n",
    "    batch_pred,batch_acc = sess.run([tf.argmax(final_pred,1),\n",
    "                                     accuracy],\n",
    "                                    feed_dict={_inputs:x_test,\n",
    "                                               _labels:y_test,\n",
    "                                               _seqlens:seqlen_test})\n",
    "    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))   \n",
    "    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quick hands-on\n",
    "* Extract the trained embedding vectors\n",
    "* Choose a word (from \"one\" to \"nine\") and find its nearest neighbors -- sort all the other word vectors by their distance to the target word. Use numpy or sklearn.\n",
    "* Re-run the network, this time without training the word vectors beyond their initial values from GloVe. Do you get different nearest neighbors?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
